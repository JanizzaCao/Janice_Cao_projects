# k-Nearest-Neighbors (kNN) classification algorithm
## 基础理论
1. 思想简述：每个样本可以用离他最近的k个邻居来代表
2. 步骤：
    1. 计算测试对象到训练集中每个对象的距离
    2. 按距离远近排序
    3. 选取与当前测试对象最近的k个训练对象，作为该测试对象的邻居
    4. 统计k个邻居的类别频次
    5. K个邻居例频次最高的类别为测试对象的类别

## 实现
1. [自实现代码](knn_code.py)
2. [sklearn.classification.KNeighborsClassifier](knn_sklearn.ipynb)

## 优缺点
|优点|缺点|
|-|-|
|1. 理论简单，可以做分类、回归，可解决多分类问题；<br> 2. 与朴素贝叶斯等算法相比，对数据没有假设，对异常点不敏感 <br> 3. 对于类域交叉、重叠较多的待分样本更合适|1. 计算量大，效率低 <br> 2. 高度数就相关、样本不平衡时，对稀有类别预测准确率低 <br> 3. 与决策树相比，可解释性不强 <br> 4. 维度灾难:随着维度增加，看似相近的两个点距离越来越大（例：100\*100像素的黑白灰图片有一万维）|

## 模型优化与发展
### KD树(待补充)
